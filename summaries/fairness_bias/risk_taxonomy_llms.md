# Taxonomy of Risks Posed by Language Models
- **Published**: FAccT ’22
- **Link**: https://doi.org/10.1145/3531146.3533088
- **Summary**: This paper develops a comprehensive taxonomy of ethical and social risks associated with large-scale language models (LMs). It identifies twenty-one risks and categorizes them into six risk areas to guide responsible innovation and mitigation strategies.

### Problem 
- Large-scale LMs pose various ethical and social risks that need to be anticipated, understood, and mitigated to ensure responsible innovation.
- Existing research has identified potential risks, but no comprehensive taxonomy has been proposed to systematically evaluate these risks.

### Contributions
- **Taxonomy**: Development of a comprehensive taxonomy categorizing twenty-one ethical and social risks associated with LMs into six areas:
  - Discrimination, Hate Speech, and Exclusion
  - Information Hazards
  - Misinformation Harms
  - Malicious Uses
  - Human-Computer Interaction Harms
  - Environmental and Socioeconomic Harms
- **Risk Analysis**: Detailed analysis of observed and anticipated risks, discussing their causal mechanisms, evidence, and mitigation approaches.
- **Guiding Action**: The taxonomy aims to facilitate the identification and mitigation of risks, contributing to responsible decision-making in LM development.

### Method
- **Interdisciplinary Workshops**: Conducted horizon-scanning workshops with researchers from various fields to surface observed and anticipated risks.
- **Literature Review**: Supplemented workshop findings with an in-depth literature review from computer science, linguistics, social sciences, and policy papers.
- **Taxonomy Development**: Analyzed common themes to structure the risk landscape into six categories.

### Result
- **Utility**: The taxonomy serves to map out challenges in LM research, inform public discourse, and support responsible decision-making.
- **Observed and Anticipated Risks**: Provided a detailed account of observed risks and a coarser overview of anticipated risks, with a focus on potential harm, evidence, and mitigation strategies.

### Limitations and Assumptions of this paper
- Focuses on risks associated with operating LMs, not on training or multimodal models.
- Primarily discusses current state-of-the-art LMs, with some extrapolation to future risks based on existing trends.

### Conclusion
- The taxonomy provides a structured framework to understand and address the ethical and social risks of LMs.
- Emphasizes the need for continuous engagement with multiple perspectives and communities for effective risk mitigation.
- Highlights the responsibility of organizations to implement the proposed mitigations and contribute to responsible LM development.

### Future work
- Engage further perspectives to expand the taxonomy.
- Innovate on analysis and evaluation methods.
- Develop and stress-test mitigation tools for responsible LM innovation.

### Reference
- Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2022. Taxonomy of Risks posed by Language Models. In Proceedings of 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22). ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3531146.3533088


# List of all categories

## 2.1 Discrimination, Hate Speech and Exclusion
- 2.1.1 Social stereotypes and unfair discrimination
- 2.1.2 Hate speech and offensive language
- 2.1.3 Exclusionary norms
- 2.1.4 Lower performance for some languages and social groups

## 2.2 Information Hazards
- 2.2.1 Compromising privacy by leaking sensitive information
- 2.2.2 Anticipated risks
  - Compromising privacy or security by correctly inferring sensitive information

## 2.3 Misinformation Harms
- 2.3.1 Disseminating false or misleading information
- 2.3.2 Causing material harm by disseminating false or poor information (e.g., in medicine or law)

## 2.4 Malicious Uses
- 2.4.1 Making disinformation cheaper and more effective
- 2.4.2 Anticipated risks
  - Assisting code generation for cybersecurity threats
  - Facilitating fraud, scams and targeted manipulation
  - Illegitimate surveillance and censorship

## 2.5 Human-Computer Interaction Harms
- 2.5.1 Promoting harmful stereotypes by implying gender or ethnic identity
- 2.5.2 Anticipated risks
  - Anthropomorphizing systems can lead to overreliance or unsafe use
  - Avenues for exploiting user trust and accessing more private information
  - Human-like interaction may amplify opportunities for user nudging, deception or manipulation

## 2.6 Environmental and Socioeconomic Harms
- 2.6.1 Environmental harms from operating LMs
- 2.6.2 Anticipated risks
  - Increasing inequality and negative effects on job quality
  - Undermining creative economies
  - Disparate access to benefits due to hardware, software, skill constraints
