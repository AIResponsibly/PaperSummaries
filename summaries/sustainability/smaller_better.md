# Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling

- **Published**: ArXiv, August 2024
- **Link**: [ArXiv](https://arxiv.org/abs/2408.16737)
- **Summary**: The paper investigates whether training language models on synthetic data generated by weaker but cheaper models (WC) is more compute-optimal than using stronger, more expensive models (SE). The study evaluates coverage, diversity, and false positive rates, showing that WC models often provide better training data under fixed compute budgets.

## I. INTRODUCTION

- **Problem**: High-quality synthetic data from strong LMs is commonly used to improve LM reasoning. However, this approach may not be compute-optimal.
- **Objective**: Explore the trade-offs between generating data using stronger, more expensive (SE) models and weaker, cheaper (WC) models.
- **Key Findings**: 
  - WC-generated data can have higher coverage and diversity but also higher false positive rates.
  - Fine-tuning on WC-generated data often outperforms SE-generated data across various benchmarks.

### Contributions

- Analysis of synthetic data generation trade-offs under fixed compute budgets.
- Introduction of a novel weak-to-strong improvement setup where weaker LM teaches reasoning to a stronger LM.
- Empirical results showing that WC-generated data can be more effective for training advanced LM reasoners.

## II. PRELIMINARIES

- **Synthetic Data Generation**: 
  - Multiple candidate solutions are sampled from an LM, filtered for correctness, and used for fine-tuning.
  - Metrics: coverage (unique problems solved), diversity (unique solutions per problem), and false positive rate (correct final answer but incorrect reasoning).

## III. COMPUTE-MATCHED SAMPLING AND TRAINING

- **Setup**:
  - Fixed sampling budget to compare SE and WC models.
  - WC models allow for more samples due to lower computational costs.
- **Finetuning Paradigms**:
  - **Knowledge Distillation**: Student LM learns from a teacher LM.
  - **Self-Improvement**: LM learns from its own generated data.
  - **Weak-to-Strong Improvement**: Strong LM improves using data from a weaker model.

## IV. EXPERIMENTAL SETUP

- **Datasets**: 
  - MATH and GSM-8K datasets for mathematical problem-solving.
- **Models**: 
  - Synthetic data generated using Gemma2-9B (WC) and Gemma2-27B (SE).
- **Evaluation**:
  - Coverage and diversity measured at fixed compute budgets.
  - False positive rates evaluated through human and automatic assessments.

## V. EXPERIMENTS AND RESULTS

### 5.1 Synthetic Data Analysis

- **Coverage**:
  - WC data outperforms SE data in coverage, especially at higher sampling budgets.
- **Diversity**:
  - WC data shows higher diversity than SE data, leading to more unique correct solutions.
- **False Positive Rate (FPR)**:
  - WC data has a higher FPR than SE data, indicating more incorrect reasoning paths despite correct answers.

### 5.2 Compute-Optimality Results for Training

- **Student-LM Finetuning**: 
  - Models trained on WC data outperform those trained on SE data.
- **WC-LM Finetuning**: 
  - Self-improvement on WC data yields better results than distillation from SE data.
- **SE-LM Finetuning**: 
  - Training SE models with WC data can be more effective than using their own data.

### 5.3 Ablation Studies
- Impact of different factors on the effectiveness of using weaker, cheaper models (WC) for training:
  - **Dataset Size**: 
    - Benefits of WC data hold across different dataset sizes.
  - **Sampling Strategy**: 
    - Compute-matched sampling from WC models is more effective than number-matched sampling.
  - **Coverage and Diversity**: 
    - High coverage and diversity in WC data are critical for training strong reasoners.

## VI. SCALING TO STATE-OF-THE-ART LANGUAGE MODELS

- **Experiment**: 
  - Compared data from Gemini-1.5-Pro (SE) and Gemini-1.5-Flash (WC) models.
- **Findings**:
  - WC data outperforms SE data, even when the WC data is generated at a fraction of the cost.

## VII. A FUTURE PERSPECTIVE

- **Observation**: 
  - Smaller LMs are improving faster than larger LMs, making the findings of this study increasingly relevant.
  
## VIII. RELATED WORK

- **LMs for Reasoning**:
  - Prior work focuses on enhancing LMs' reasoning capabilities through prompting or fine-tuning.
- **Finetuning LMs**:
  - Common methods involve knowledge distillation from strong LMs or self-improvement.
  - "we explore the utility of the synthetic data from the weak but cheap LMs for training strong reasoners via knowledge distillation as well as self-improvement."
  - "we do not explore using model-based verifiers with the synthetic data for enhanced reasoning, and leave it as a future work."

- **Large and Small LMs**:
  - Recent interest in capable small LMs due to their efficiency and scalability.

## IX. CONCLUSION

- **Summary**: 
  - Sampling from weaker, cheaper LMs may be more compute-optimal for training LM reasoners.
  - WC data often provides better coverage and diversity under fixed compute budgets.
- **Implication**: 
  - This approach challenges conventional practices and could shape future LM training strategies.

## REFERENCE

Daniel Freeman, Haonan Yu, Jiaming Song, Yao Shu, Daniel D. Johnson, Bo Li, and Ahmed Aly. "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling," arXiv preprint arXiv:2408.16737, 2024.
