# A Grading Rubric for AI Safety Frameworks

- **Published**: ArXiv, September 2024
- **Authors**: Jide Alaga, Jonas Schuett, Markus Anderljung
- **Affiliation**: Centre for the Governance of AI
- **Link**: [ArXiv](https://arxiv.org/abs/2409.08751v1)
- **Summary**: This paper proposes a comprehensive grading rubric for evaluating AI safety frameworks. It introduces seven evaluation criteria with 21 indicators, a six-tier grading system, and three methods for applying the rubric. The goal is to enable nuanced comparisons between frameworks, identify areas for improvement, and promote responsible AI development.

## I. INTRODUCTION

- **Problem**: AI companies are increasingly adopting safety frameworks, but there's no standardized way to evaluate their effectiveness.
- **Objective**: Develop a grading rubric to enable governments, academia, and civil society to assess AI safety frameworks.
- **Key Components**:
  - Seven evaluation criteria
  - 21 indicators to concretize the criteria
  - Six-tier grading system from A (gold standard) to F (substandard)
  - Three methods for applying the rubric: surveys, Delphi studies, and audits

### 1.1 What are AI safety frameworks?

- **Definition**: Risk management policies to keep potential risks from frontier AI systems at an acceptable level
- **Focus**: Primarily address AI-enabled catastrophic risks (e.g., misuse of chemical/biological weapons, cyberattacks, loss of control)
- **Main Components**:
  1. Risk identification: Analyze potential catastrophic outcomes
  2. Risk assessment: Gather evidence about system capabilities
  3. Risk mitigation: Specify safety measures for different capability levels
  4. Risk governance: Ensure adherence and maintain effectiveness

### 1.2 The case for passing judgment on AI safety frameworks

- **Identifying shortcomings**: Grading process can reveal areas for improvement
- **Incentivizing improvement**: Poor grades may motivate companies to enhance their frameworks
- **Preparing for regulation**: Developing assessment skills for potential future regulatory requirements
- **Informing public discussion**: Provide transparent evaluations of company commitments to safety

### 1.3 Related work

- **Existing safety frameworks**:
  - Anthropic's Responsible Scaling Policy (September 2023)
  - OpenAI's Preparedness Framework (Beta) (December 2023)
  - Google DeepMind's Frontier Safety Framework (May 2024)
  - Magic's AGI Readiness Policy (July 2024)
  - 13 additional companies committed to producing frameworks by February 2025

- **Recommendations for safety frameworks**:
  - METR outlined five main components
  - UK DSIT proposed seven practices for responsible capability scaling
  - Frontier AI Safety Commitments listed key elements

- **Reviews of existing frameworks**:
  - Anderson-Samways et al. evaluated Anthropic's RSP against DSIT guidance
  - Ó hÉigeartaigh et al. conducted a rapid review of company statements
  - SaferAI compared Anthropic's RSP with OpenAI's Preparedness Framework

- **Evaluation criteria**:
  - Titus proposed nine criteria for robustly addressing risks in advanced AI systems
  - This paper builds on previous work by presenting criteria as evaluation tools

## II. GRADING RUBRIC FOR AI SAFETY FRAMEWORKS

### 2.1 Effectiveness

#### 2.1.1 Credibility
- **Indicators**: 
  - Causal pathways: Clear explanations of how the framework keeps risks acceptable
  - Empirical evidence: Support from controlled experiments or relevant case studies
  - Expert opinion: Agreement from relevant AI safety and governance experts

#### 2.1.2 Robustness
- **Indicators**:
  - Safety margins: Implementing measures before necessary or stronger than required
  - Redundancies: Using multiple, complementary methods for risk assessment and mitigation
  - Stress testing: Anticipating potential failure modes and worst-case scenarios
  - Revisions: Continual review and updates to reflect state-of-the-art practices

### 2.2 Adherence

#### 2.2.1 Feasibility
- **Indicators**:
  - Commitment difficulty: Inherent challenge of implementing proposed measures
  - Developer competence: In-house knowledge, skills, and experience
  - Resources committed: Allocation of time, funding, and talent

#### 2.2.2 Compliance
- **Indicators**:
  - Ownership: Clear assignment of responsibilities
  - Incentives: Rewards for compliance and consequences for non-compliance
  - Monitoring: Tracking adherence to commitments
  - Oversight: Specific individuals or teams responsible for overseeing implementation

#### 2.2.3 Empowerment
- **Indicators**:
  - Access to resources: Availability of necessary tools and information
  - Autonomy: Protection from interference by actors with competing interests

### 2.3 Assurance

#### 2.3.1 Transparency
- **Indicators**:
  - Clarity: Understandable, precise, concise, and consistent language
  - Comprehensiveness: Inclusion of all key elements and implementation details
  - Rationales: Explanations and justifications for key design choices

#### 2.3.2 External scrutiny
- **Indicators**:
  - Expert review: Independent evaluation of the framework's effectiveness
  - Implementation audits: Third-party verification of proper implementation

### 2.4 Quality tiers

- **A-class (Gold Standard)**: Fully satisfies criteria, marginal room for improvement
- **B-class**: Mostly satisfies criteria, minor areas for improvement
- **C-class**: Satisfies core aspects, notable areas for improvement
- **D-class**: Partially satisfies criteria, major areas for improvement
- **E-class**: Minimally satisfies criteria, significant weaknesses
- **F-class (Substandard)**: Completely fails to satisfy criteria, needs improvement in almost every part

## III. HOW TO APPLY THE GRADING RUBRIC

### 3.1 Survey
- **Process**: 
  - Design survey asking participants to evaluate each criterion
  - Send to independent AI safety and governance experts
  - Aggregate responses and report average grades
- **Advantages**: Less resource-intensive, clear output
- **Limitations**: May not capture all nuances and context-specific factors

### 3.2 Delphi study
- **Process**:
  - Initial survey
  - Aggregation of responses and anonymized summaries
  - Workshop discussions
  - Opportunity to update responses
  - Repeat until consensus (optional)
- **Advantages**: Leverages expert insights, encourages consensus building
- **Limitations**: Time-consuming, potential for groupthink

### 3.3 Audit
- **Process**:
  - Commission independent experts or firms
  - Provide access to non-public information and key personnel
  - Conduct interviews, review documents, and perform on-site evaluations
- **Advantages**: Comprehensive understanding, access to confidential information
- **Limitations**: Time-consuming, costly, relies on company cooperation

## IV. LIMITATIONS

1. Grading results might not translate into actionable recommendations
2. Many criteria are difficult to measure objectively
3. Evaluation requires specialized AI safety expertise
4. Evaluation criteria are unlikely to be exhaustive
5. Difficulty in differentiating between the six quality tiers
6. Lack of weighting for evaluation criteria

## V. CONCLUSION

- **Main Contribution**: Comprehensive grading rubric for AI safety frameworks
- **Implications**: 
  - Enables nuanced comparisons between frameworks
  - Helps identify areas for improvement
  - Promotes a race to the top in responsible AI development
- **Future Work**: 
  - Refinement of the rubric based on practical application
  - Development of more easily assessed indicators
- **Call to Action**: Encourages use by governments, researchers, and civil society organizations
